# -*- coding: utf-8 -*-
"""dl_deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fKwws0_aPZCti-PDnkdy4aT1axaKQqpa
"""

import sys
sys.tracebacklimit=0
import numpy as np
import streamlit as st
import librosa
import librosa.display
from tensorflow.keras.models import load_model
import os
import streamlit.components.v1 as components
import warnings
warnings.filterwarnings('ignore', '.*do not.*', )
warnings.warn('DelftStack')
warnings.warn('Do not show this message')
from PIL import Image

paths = []
labels = []
for dirname, _, filenames in os.walk('TESS Toronto emotional speech set data'): # ignore:type
    for filename in filenames:
        paths.append(os.path.join(dirname, filename))
        label = filename.split('_')[-1]
        label = label.split('.')[0]
        labels.append(label.lower())
    if len(paths) == 2800:
        break

## Create a dataframe
import pandas as pd

df = pd.DataFrame()
df['speech'] = paths
df['label'] = labels

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
enc.fit_transform(df[['label']])

side_img = Image.open("emotions.png")
with st.sidebar:
  st.image(side_img, width=300)

st.title('Speech Emotion Recognition Model Deployment')# type:ignore

# load models
model = load_model("model3.h5")# type:ignore
audio_file = st.file_uploader("Upload audio file", type=['wav', 'mp3', 'ogg'])# type:ignore
st.audio(audio_file, format='audio/wav', start_time=0)# type:ignore

st.write('Althouth the NameError stating that mfcc has not been defined appears, the model works, you can go ahead by uploading a sample audion file and detect the emotion, some of them are present in the folder.')

#feature extraction
try:
  y, sr = librosa.load(audio_file, duration=3, offset=0.5)
  mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
except:
  pass

X_mfcc=mfcc

X = np.array(X_mfcc)

## input split
X = np.expand_dims(X, -1)

X = np.expand_dims(X, 0)

#prediction
pred_test = model.predict(X)
y_pred = enc.inverse_transform(pred_test)
#the final output is y_pred
result=y_pred[0,0]

st.subheader('Emotion Detected in the Uploaded Audio File: ')
st.write(result.upper())

